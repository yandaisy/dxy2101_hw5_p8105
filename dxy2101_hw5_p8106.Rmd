---
title: "dxy2101_hw5_p8106"
author: "Daisy Yan"
date: "2022-11-10"
output: github_document
---

```{r setup, message=FALSE}
library(tidyverse)
library(ggplot2)

set.seed(1)
```

## Problem 1

Create a tidy dataframe containing data from all participants, including the subject ID, arm, and observations over time.

```{r study, message=FALSE}
# Load data
study_files =
  tibble(files = list.files("./data")) %>%
  mutate(files = str_c("data", files, sep = "/"))

study_data =
  study_files %>%
  mutate(subjects = map(files, read_csv)) %>%
  mutate(arm = case_when(str_detect(files, "exp") ~ "experiment",
                         str_detect(files, "con") ~ "control"),
         sub_id = as.factor(parse_number(files))) %>%
  unnest(subjects) %>%
  pivot_longer(cols = week_1:week_8, names_to = "week", values_to = "observations") %>%
  mutate(week = as.numeric(parse_number(week)))
```

Make spaghetti plot.

```{r pasta}
study_data %>%
  ggplot(aes(x = week, y = observations, color = sub_id)) +
  geom_line() + ylab("Observations") + xlab("Weeks") +
  facet_grid(cols = vars(arm)) +
  ggtitle("Observations of Subjects Over Time")
```

In the control group, subjects report similar observations over time. On the other hand, in the experimental group, subjects reeport higher observations over time.

## Problem 2

Describe raw dataset.

```{r homicides, message=FALSE}
# Load data
url = 'https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv'

homicide_data =
  read_csv(url)
```

The homicide data set contains `r nrow(homicide_data)` observations and `r ncol(homicide_data)` variables. The variables included are the following: unique id, reported date, victim characteristics (last name, first name, race, age, sex), location of case (city, state, latitude, longitude), and disposition of case.

Create `city_state` variable. Summarize within cities to obtain number of solved homicides and unsolved homicides.

```{r city, message=FALSE}
# Create new variables and clean
homicide_data =
  homicide_data %>%
  mutate(city_state = str_c(city, state, sep = ", "),
         status = case_when(
           disposition == "Closed without arrest" ~ "unsolved",
           disposition == "Open/No arrest" ~ "unsolved",
           disposition == "Closed by arrest" ~ "solved"
         )) %>%
  select(-city, -state, -disposition) %>%
  relocate(city_state, .before = lat)

# Summarize
status_summary =
  homicide_data %>%
  group_by(city_state, status) %>%
  summarise(count = n())
```

Estimate proportion of unsolved homicides in Baltimore, MD.

```{r baltimore}
# Filter to Baltimore
baltimore_data =
  homicide_data %>%
  filter(city_state == "Baltimore, MD") %>%
  summarise(
    unsolved = sum(status == "unsolved"),
    total = n()
  )

# prop.test
baltimore_prop =
  prop.test(
    x = baltimore_data$unsolved,
    n = baltimore_data$total
  )

baltimore_estimate =
  baltimore_prop %>%
  broom::tidy()
```

Estimate proportion of unsolved homicides in all cities.

```{r all cities, message=FALSE, warning=FALSE}
# Create function to run prop.test
prop_function = function(cities){
  
  cities_data  =
    cities %>%
    summarise(
      unsolved = sum(status == "unsolved"),
      total = n()
    )
  
  cities_prop =
    prop.test(
      x = cities_data %>% pull(unsolved),
      n = cities_data %>% pull(total)
    )
  
  cities_prop
}

# Test prop_function
prop_function(
  homicide_data%>%
  filter(city_state == "Baltimore, MD"))

# Iterate across all cities
results = 
  homicide_data %>% 
    nest(-city_state) %>% 
    mutate(
      test = map(data, prop_function),
      tidy = map(test, broom::tidy)
    ) %>% 
    select(city_state, tidy) %>% 
    unnest(tidy) %>% 
    select(city_state, estimate, starts_with('conf'))

results
```

Plot estimates of CIs for each city.

```{r plot CI}
results %>%
  mutate(
    city_state = fct_reorder(city_state, estimate)
  ) %>%
  ggplot(aes(x = city_state, y = estimate)) + geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
  xlab("City, State") + ylab("Proportion of Unsolved Homicides") +
  ggtitle("Estimated Proportion of Unsolved Homicides by City")
```

## Problem 3

Set design elements.

```{r set}
# Generate 5000 datasets
set_1 = rerun(5000, rnorm(n = 30, mean = 0, sd = 5))
```

For each data set, save mu and the p-value.

```{r ttest}
# Create function to run t test
t_test = function(mu = 0){
  
  sim_data = tibble(
    x = rnorm(n = 30, mean = mu, sd = 5),
  )
  
  output = t.test(sim_data) %>% 
    broom::tidy() %>% 
    select(estimate, p.value)
  
  output
}

# Test t_test
sim_results_df = 
  expand_grid(
    mean = 0,
    iter = 1:5000
  ) %>% 
  mutate(
    estimate_df = map(mean, t_test)
  ) %>% 
  unnest(estimate_df)

# Repeat process for mu={1,2,3,4,5,6}
sim_results_multi = 
  expand_grid(
    mean = 1:6,
    iter = 1:5000
  ) %>% 
  mutate(
    estimate_df = map(mean, t_test)
  ) %>% 
  unnest(estimate_df)
```

Make a plot showing the proportion of times the null was rejected (the power of the test) on the y axis and the true value of mu on the x axis.

```{r plot power}
# Plot power
sim_results_multi %>%
  group_by(mean) %>% 
  summarize(prop_rej = sum(p.value < 0.05)/5000) %>% 
  ggplot(aes(x = mean,y = prop_rej)) + geom_point() + geom_path() +
  xlab("Mean") + ylab("Power") +
  ggtitle("Power of T Test by Different Means")
```

Effect size is typically used to determine the magnitude of difference between two groups. As effect size increases, power increases. This is supported by the plot above.

Now, make a plot showing the average estimate of mu on the y axis and the true value of mu on the x axis. Overlay the average estimate of mu only in samples for which the null was rejected on the y axis and the true value of mu on the x axis.

```{r plot mu}
# Average estimate of mu
all = 
  sim_results_multi %>%
  group_by(mean) %>% 
  summarize(average = mean(estimate, na.rm = TRUE))

# Average estimate of mu - null rejected
rejected = 
  sim_results_multi %>%
  filter(p.value < 0.05) %>%
  group_by(mean) %>% 
  summarize(average = mean(estimate, na.rm = TRUE))

# Plot 
ggplot(all, aes(x = mean, y = average)) +
  scale_color_manual(name = "Estimates of Mu", values = c("green" = "green", "purple" = "purple"), labels = c("All", "Rejected")) +
  geom_point(data = all, aes(colour = "green")) +
  geom_line(data = all, aes(colour = "green")) +
  geom_point(data = rejected, aes(colour = "purple")) +
  geom_line(data = rejected, aes(colour = "purple")) +
  xlab("True Mean") + ylab("Average Estimate of Mean") +
  ggtitle("True Mean vs. Average Estimate of Mean")
```

The sample average of mu across tests for which the null is rejected is not equal to the true value of mu. This is because for smaller effect sizes, there is a smaller power (the probability that a false null hypothesis is rejected). As we see in the graph above, as effect size increases, the sample average of mu across tests for which the null is rejected aligns closer in value to the true value of mu.
